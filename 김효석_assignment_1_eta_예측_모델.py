# -*- coding: utf-8 -*-
"""김효석_Assignment_1_ETA_예측_모델

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1LVMiecNL4WaShgJhW3nIpqtr4SIkmduC
"""

from google.colab import drive
drive.mount('/content/drive')

# Commented out IPython magic to ensure Python compatibility.
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
# %matplotlib inline

tada_eta = pd.read_excel('/content/drive/MyDrive/Colab Notebooks/data/tada_eta.xlsx')
print(tada_eta.columns)
tada_eta.head()

# pickup_gu를 categorical feature로 만들기
import sklearn.preprocessing
enc = sklearn.preprocessing.OrdinalEncoder(dtype=np.int32)
ordinal = enc.fit_transform(np.asarray(tada_eta['pickup_gu']).reshape(-1,1))
tada_eta['pickup_gu'] = ordinal[:,0]

tada_eta['distance'] = ((tada_eta['pickup_lat']-tada_eta['driver_lat'])**2 + (tada_eta['pickup_lng']-tada_eta['driver_lng'])**2)*100000
tada_eta = tada_eta.drop(['id', 'created_at_kst', 'driver_id', 'pickup_lng', 'pickup_lat', 'driver_lng','driver_lat'],1)
tada_eta.head()

tada_eta = tada_eta.sample(frac=1, random_state=0).reset_index(drop=True)

tada_eta.head()

train = tada_eta[:12000]
test = tada_eta[12000:]
train

# api_eta와 distance만 정규화하기 위한 데이터 수정
x_train = np.asarray(train.drop(['ATA', 'month', 'hour', 'pickup_gu'], 1))
y_train = np.asarray(train['ATA'])

x_test = np.asarray(test.drop(['ATA', 'month', 'hour', 'pickup_gu'], 1))
y_test = np.asarray(test['ATA'])

month_train = np.asarray(train['month'])
month_test = np.asarray(test['month'])
hour_train = np.asarray(train['hour'])
hour_test = np.asarray(test['hour'])
pickup_gu_train = np.asarray(train['pickup_gu'])
pickup_gu_test = np.asarray(test['pickup_gu'])

eta_features = [x for i, x in enumerate(tada_eta.columns) if i!=0]

# 정규화
from sklearn.preprocessing import MinMaxScaler

scaler = MinMaxScaler()
x_train = scaler.fit_transform(x_train)
x_test = scaler.transform(x_test)

x_train

# 정규화한 x_train 및 x_test와 month, hour, pickup_gu 데이터 이어 붙이기
x_train = pd.DataFrame(x_train)
x_train['month'] = month_train
x_train['hour'] = hour_train
x_train['pickup_gu'] = pickup_gu_train

x_test = pd.DataFrame(x_test)
x_test['month'] = month_test
x_test['hour'] = hour_test
x_test['pickup_gu'] = pickup_gu_test

x_train = np.asarray(x_train)
x_test = np.asarray(x_test)

x_train

from sklearn.metrics import mean_squared_error
from sklearn.metrics import mean_absolute_error
from sklearn.ensemble import HistGradientBoostingRegressor

from sklearn.model_selection import GridSearchCV
from sklearn.model_selection import PredefinedSplit

param_grid = {'max_depth': [2, 3, 4, 5],
              'min_samples_leaf': [30, 35, 40],
              'learning_rate':[0.05, 0.1, 0.5],
              'max_iter': [60, 65, 70, 75, 80]
              }

# train set과 test set의 input과 output를 각각 이어 붙여서 X,y를 정의
X = np.concatenate((x_train,x_test), axis=0)
y = np.concatenate((y_train,y_test), axis=0)

# 전체 데이터 X에서 training data에 해당하는 index는 -1, test data에 해당하는 index는 0이 되도록,
# 여기서는 [-1, -1, ... , -1, 0, 0, ... , 0] 같은 형태의 1차원 배열 test fold와 predefined split을 정의
pds = PredefinedSplit(test_fold=[-1]*len(x_train)+[0]*len(x_test))

# grid search 모델 정의, 학습 및 model selection
grid_search = GridSearchCV(estimator=HistGradientBoostingRegressor(random_state=0, categorical_features=[2, 3, 4]), 
                           param_grid=param_grid, 
                           cv=pds, n_jobs=-1, scoring='neg_mean_squared_error', verbose=2)
grid_search.fit(X, y)
print(grid_search.best_params_)

reg = HistGradientBoostingRegressor(learning_rate=0.1, max_depth=2, min_samples_leaf=35, max_iter=70, random_state=0, categorical_features=[2, 3, 4])
reg.fit(x_train, y_train)

mse = mean_squared_error(y_test, reg.predict(x_test))
print("The mean squared error (MSE) on test set: {:.4f}".format(mse))
print("The initial error of API ETA on test set: {:.4f}".format(mean_squared_error(y_test, x_test[:,0]) ))

mae = mean_absolute_error(y_test, reg.predict(x_test))
print("The mean absolute error (MAE) on test set: {:.4f}".format(mae))
print("The initial error of API ETA on test set: {:.4f}".format(mean_absolute_error(y_test, x_test[:,0]) ))

from sklearn.inspection import permutation_importance
result = permutation_importance(reg, x_train, y_train)
sorted_idx = result.importances_mean.argsort()

fig, ax = plt.subplots()
ax.boxplot(
    result.importances[sorted_idx].T, vert=False, labels=np.array(eta_features)[sorted_idx]
)
ax.set_title("Permutation Importances (train set)")
fig.tight_layout()
plt.show()

